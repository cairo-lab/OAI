{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "innocent-content",
   "metadata": {},
   "source": [
    "# Parse the VG_Form.pdf for OAI variable categories and sources\n",
    "\n",
    "OAI has a number of PDFs that list all variables collected, their nature, and where they came from. Most handy is General Information.zip\\/General\\/VG_Form.pdf as we can use this to get the categories and subcategories and assemble them into a readable table.\n",
    "\n",
    "With over 9,000 variables collected, this is critical just to start to make sense of the variables.\n",
    "\n",
    "For now, this only extracts a wiki table for categories/subcategories of the variables collected each visit, and another for tracking what sources were used for which visit. Clearly, more can be done with the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "copyrighted-begin",
   "metadata": {},
   "source": [
    "## Imports and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dated-credit",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas\n",
    "from IPython.display import display\n",
    "from string import ascii_letters\n",
    "from pdfminer.high_level import extract_pages\n",
    "from pdfminer.layout import LTTextContainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reliable-ordering",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "# Tags to look for when parsing \n",
    "hdr_tag = \"Variable Guide\"\n",
    "vn_tag = \"Variable Name\"\n",
    "var_tag = \"Variable\"\n",
    "src_tag = \"Source\"\n",
    "page_tag = \"Page\"\n",
    "lbl_tag = \"Label\"\n",
    "sas_ds_tag = \"SAS Dataset\"\n",
    "rc_tag = \"Release Comments\"\n",
    "cat_tag = \"Category\"\n",
    "sub_tag = \"Subcategory\"\n",
    "stats1_tag = \"N \"\n",
    "stats2_tag = \"Value N\"\n",
    "end_tag = \"_______________\"\n",
    "ftr_tag = \"Release Version\"\n",
    "\n",
    "# This list was created by first parsing and collecting all listed categories into a set\n",
    "known_categories = [\"Bookkeeping\", \"Demographics\", \"Study eligibility\", \"Knee symptoms\",\n",
    "                    \"Medical history, arthritis\", \"Medications\", \"Physical activity\",\n",
    "                    \"Medical history, general Comorbidity\", \"Knee pain/OA status\",\n",
    "                    \"Knee function/QOL Knee\", \"Other joint symptoms\", \"Back pain\",\n",
    "                    \"Anthropometry\", \"Hand and/or foot exam\", \"Medical history, general\", \n",
    "                    \"Image Assessments: X-ray\", \"Joint imaging\", \"Health care access\",\n",
    "                    \"Global function/disability/QOL\", \"Nutrition\", \"WOMAC/KOOS\", \n",
    "                    \"Knee function/QOL\", \"Blood pressure & pulse\", \"Performance measures\", \n",
    "                    \"Strength measures\", \"Knee exam\", \"Biospecimens collection\",\n",
    "                    \"Image Assessments: MRI\", \"Accelerometry\", \"Hip function/QOL\", \n",
    "                    \"Hip symptoms\", \"Outcomes\"]\n",
    "\n",
    "visits = {'P02':'IEI', 'P01':'SV', 'V00':'EV', 'V01':'12m', 'V02':'18m', 'V03':'24m', 'V04':'30m', 'V05':'36m', 'V06':'48m', 'V07':'60m', 'V08':'72m', 'V09':'84m', 'V10':'96m', 'V11':'108m', 'V99':\"Outcomes\"}\n",
    "\n",
    "col_names = [var_tag, lbl_tag, src_tag, page_tag, sas_ds_tag, rc_tag]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joint-taylor",
   "metadata": {},
   "source": [
    "## Original file ingestion\n",
    "\n",
    "Parse the PDF once, and convert into a smaller serialized format for faster re-processing in the future (approx 10 min to read the whole PDF). Do this once, and afterwards only use the binary reader in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lyric-vegetarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse PDF into a list of pages\n",
    "# Each page is only a list of the PDF textboxes in the page\n",
    "\n",
    "pages = []\n",
    "for page_layout in tqdm(extract_pages(r\"..\\..\\Downloads\\General Information\\General\\VG_Form.pdf\")):\n",
    "    text_boxes = []\n",
    "    for element in page_layout:\n",
    "        if isinstance(element, LTTextContainer):\n",
    "            text_boxes.append((element.get_text(), element.x0, element.y0, element.x1, element.y1))\n",
    "    pages.append(text_boxes)\n",
    "\n",
    "pickle.dump(pages, open( \"vg_form_pdf_elements.pkl\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposite-cleaning",
   "metadata": {},
   "source": [
    "# Fast file ingestion\n",
    "Read in the binary serialized data (approx 1 sec to read the binary form) created in prior section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinguished-milan",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = pickle.load(open(\"vg_form_pdf_elements.pkl\", \"rb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streaming-customs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many variables are described in this document? (for a later sanity check)\n",
    "total_var_cnt = 0\n",
    "for page in pages:\n",
    "    for element in page:\n",
    "        if element[0].strip().startswith(end_tag):\n",
    "            total_var_cnt += 1\n",
    "print(total_var_cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "destroyed-remove",
   "metadata": {},
   "source": [
    "## Ensure the PDF textboxes are in the same order they are rendered on a page "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedicated-botswana",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse PDF elements into a list of text boxes and coordinates that match their rendering order\n",
    "\n",
    "lines = []\n",
    "for page in tqdm(pages):\n",
    "    text_boxes = []\n",
    "    for element in page:\n",
    "        txt = element[0].strip()\n",
    "        \n",
    "        # found text box containing a header or footer, ignore and move on\n",
    "        if txt.startswith(hdr_tag) or txt.startswith(page_tag) or txt.startswith(ftr_tag):\n",
    "            continue\n",
    "        \n",
    "        # Check for multiple lines in a single text_box\n",
    "        txt = txt.split('\\n')\n",
    "        if len(txt) == 1:\n",
    "            text_boxes.append((txt[0].strip(), element[2], element[4], element[1])) # txt, y0, y1, x0\n",
    "        else:\n",
    "            # Delete dups: This PDF has random text repeated in the PDF even though it only renders them once\n",
    "            tmp = []\n",
    "            [tmp.append(x.strip()) for x in txt if x not in tmp]\n",
    "            txt = tmp\n",
    "            \n",
    "            # Break multiline boxes into single line boxes\n",
    "            line_cnt = len(txt)\n",
    "            height = (element[4] - element[2])/line_cnt # y1-y0\n",
    "            for i in range(line_cnt):\n",
    "                text_boxes.append((txt[i], element[2]+(height*(line_cnt-(1+i))), element[4]-(height*i), element[1]))\n",
    "\n",
    "    # Order text_boxes by vertical (top to bottom), then horizontal position (left to right)                \n",
    "    text_boxes = sorted(text_boxes, key=lambda box: box[3])  # secondary sort variable (horiz pos)\n",
    "    text_boxes = sorted(text_boxes, key=lambda box: box[2], reverse=True) # primary sort variable (vert pos)\n",
    "\n",
    "    # Concatenate text_boxes that render at same horizontal position (shows up in the pdf as Label: value)\n",
    "    last_tb = [\"\", sys.maxsize, sys.maxsize, sys.maxsize]\n",
    "    for tb in text_boxes:\n",
    "        if (last_tb[1] - tb[1]) < 1:  # same line of text on a page\n",
    "             lines[-1] = lines[-1] + \" \" + tb[0].strip()\n",
    "        else:  # new line of text\n",
    "            lines.append(tb[0].strip())\n",
    "        last_tb = tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjacent-settle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional memory cleanup\n",
    "text_boxes = None\n",
    "pages = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinate-entrance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kludge: there are a few text boxes that place the text None a line before Release Comments,\n",
    "# yet renders them on the same line as Release Comments: None\n",
    "# This hunts those down and fixes them before parsing\n",
    "l = 0\n",
    "while l < len(lines):\n",
    "    if lines[l] == \"None\" and lines[l+1].startswith(rc_tag):\n",
    "        lines[l] = lines[l+1] + \" \" + lines[l]\n",
    "        lines.pop(l+1)\n",
    "    l += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pregnant-apparatus",
   "metadata": {},
   "source": [
    "## Parse text of the document into data about OAI variables\n",
    "First parse into lists, then into pandas dataframes.\n",
    "One dataframe includes each variable, label, source, source page, dataet file name, and release comment.\n",
    "Since each variable can have more than one category/subcategory associated with it, a separate dataframe is used to hold these associations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressed-postcard",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the text into data about the OAI variables\n",
    "\n",
    "def get_var(lines, l, label, next_label=None):\n",
    "    # Parse as many lines as needed to get the variable value\n",
    "    # Return value and new parser location\n",
    "    assert(lines[l].startswith(label))\n",
    "    value = lines[l][len(label)+1:].strip()\n",
    "    l += 1\n",
    "    # Some comments run beyond one line\n",
    "    while next_label and not lines[l].startswith(next_label):\n",
    "        value = value + \" \" + lines[l].strip()\n",
    "        l += 1\n",
    "    return value, l\n",
    "    \n",
    "\n",
    "variables = []\n",
    "variable_cats = [] \n",
    "l = 0\n",
    "while l < len(lines):\n",
    "    # The first line describes where the variable came from\n",
    "    src_page = None\n",
    "    src = lines[l].strip()\n",
    "    # If applicable, split out src and src_page\n",
    "    idx = re.search(page_tag + \"|\" + page_tag.lower() + \"| p |, p\", lines[l])\n",
    "    if idx:\n",
    "        src = lines[l][:idx.start()].strip()\n",
    "        src_page = lines[l][idx.end():].strip()\n",
    "    else:\n",
    "        idx = re.search(\"p\\d+\", lines[l]) # e.g. p50Q\n",
    "        if idx:\n",
    "            src = lines[l][:idx.start()].strip()\n",
    "            src_page = lines[l][idx.start()+1:].strip()\n",
    "\n",
    "    # Clean up source names that are different just because of spelling\n",
    "    src = src.replace(\"Follow-Up\", \"Follow-up\")\n",
    "    src = re.sub(\"Quest$\", \"Questionnaire\", src)\n",
    "    src = src.replace(\"Checklist\", \"\").strip()\n",
    "    \n",
    "    # If src_page # exists, parse it\n",
    "    if src_page:\n",
    "        if '-' in src_page:  # e.g. 25-26\n",
    "            [src_page, _] = src_page.split('-')\n",
    "            if not src_page.isdigit(): # e.g. 6/108-Mo piv\n",
    "                [src_page, _] = src_page.split('/')\n",
    "        elif '(' in src_page:  # e.g. 8(a)\n",
    "            [src_page, _] = src_page.split('(')\n",
    "        elif src_page.isalpha():  # e.g. ii\n",
    "            src_page = 0\n",
    "        elif not src_page.isdigit(): # e.g. 27Qf\n",
    "            src_page = src_page.rstrip(ascii_letters) \n",
    "        \n",
    "        src_page = int(src_page)\n",
    "    \n",
    "    l += 1\n",
    "    \n",
    "    # Next is the variable name, never larger than a single line \n",
    "    var_name, l = get_var(lines, l, vn_tag)\n",
    "    \n",
    "    # Next is the variable label\n",
    "    label, l = get_var(lines, l, lbl_tag, sas_ds_tag)\n",
    "    \n",
    "    # Get the name of the SAS Dataset the variable is stored in\n",
    "    dataset, l = get_var(lines, l, sas_ds_tag)\n",
    "    \n",
    "    # Get the release comments\n",
    "    rel_cmnts, l = get_var(lines, l, rc_tag, cat_tag)\n",
    "    \n",
    "    # Get the categories/subcategories\n",
    "    assert(lines[l].startswith(cat_tag))\n",
    "    l += 1 # desired values are the line below the text \"Category:\"\n",
    "    while not lines[l].startswith(stats1_tag) and not lines[l].startswith(stats2_tag) and not lines[l].startswith(end_tag):\n",
    "        for cat in known_categories:\n",
    "            if lines[l].startswith(cat):\n",
    "                # Store as name, category, subcategory (which is always on the same line as the category)\n",
    "                variable_cats.append((var_name, cat, lines[l][len(cat):].strip()))\n",
    "                break\n",
    "        l += 1\n",
    "    \n",
    "    # All remaining text is the summary stats section (ignored), and the line marks the end of a variable description\n",
    "    while l < len(lines) and not lines[l].startswith(end_tag):\n",
    "        l += 1\n",
    "    l += 1\n",
    "    \n",
    "    variables.append((var_name, label, src, src_page, dataset, rel_cmnts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "external-scenario",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional memory cleanup\n",
    "lines = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selective-castle",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Put data into Pandas dataframe, and optimize the storage (reduces to 1/3 size)\n",
    "\n",
    "vars_df = pandas.DataFrame(variables, columns=col_names)\n",
    "# Setting types, reduces memory size by 50%\n",
    "vars_df[var_tag] = vars_df[var_tag].astype('string')\n",
    "vars_df[lbl_tag] = vars_df[lbl_tag].astype('string')\n",
    "vars_df[src_tag] = vars_df[src_tag].astype('category')\n",
    "vars_df[page_tag] = vars_df[page_tag].astype('UInt8')\n",
    "vars_df[sas_ds_tag] = vars_df[sas_ds_tag].astype('category')\n",
    "vars_df[rc_tag].replace('None', np.nan, inplace=True)\n",
    "vars_df[rc_tag] = vars_df[rc_tag].astype('category')\n",
    "\n",
    "vars_cat_df = pandas.DataFrame(variable_cats, columns=[var_tag, cat_tag, sub_tag])\n",
    "vars_cat_df[var_tag] = vars_cat_df[var_tag].astype('string')\n",
    "vars_cat_df[cat_tag] = vars_cat_df[cat_tag].astype('category')\n",
    "vars_cat_df[sub_tag] = vars_cat_df[sub_tag].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visible-invitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional cleanup\n",
    "variables = None\n",
    "variable_cats = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controlled-renewal",
   "metadata": {},
   "source": [
    "## Review results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brutal-stone",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  A quick look at the variable parse results\n",
    "\n",
    "# print(str(vars_df.memory_usage(index=True).sum()) + \" bytes\")\n",
    "vars_df.describe(exclude=[\"UInt8\"])  # Exclude the page column beacuse that is all it will render"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "light-subscription",
   "metadata": {},
   "source": [
    "There seem to be 27 variables that are repeated. Each repeated variable has a source listed as \"Follow-up Visit Interview/Workbook\" and a twin with a source \"96-Month Close-Out/108-Mo Invw\" (or something similar).\n",
    "\n",
    "Labels are repeated because labels don't encode the visit data, and the same questions get asked at different visits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "third-worcester",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at repeated variables\n",
    "vars_df.loc[vars_df['Variable'].duplicated(keep=False)].sort_values(by=['Variable'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sapphire-warrior",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  A quick look at the category/subcat parse results\n",
    "\n",
    "# print(str(vars_cat_df.memory_usage(index=True).sum()) + \" bytes\")\n",
    "vars_cat_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharp-engineering",
   "metadata": {},
   "source": [
    "Many variables have more than one category/subcategory assigned to them, so the count being greater than 9250/9277 is expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ignored-referral",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity checks\n",
    "\n",
    "print(\"Variables in dataframe: \" + str(vars_df.shape[0]))\n",
    "assert vars_df.shape[0] == total_var_cnt\n",
    "\n",
    "# NA is the expected columns?\n",
    "for cn in col_names:\n",
    "    na_cnt = vars_df[cn].isna().sum()\n",
    "    if na_cnt > 0 and cn not in [page_tag, rc_tag]:\n",
    "        print(\"\\n!!!Unexpected NA values in column: \" + cn)\n",
    "\n",
    "# Are the source page numbers sane?\n",
    "if vars_df.Page.max() > 80:\n",
    "    print(\"\\n!!!Unexpectedly large page number: \" + str(vars_df.Page.max()))\n",
    "\n",
    "# Are the variable names sane?\n",
    "if vars_df.Variable.map(len).max() > 11:\n",
    "    print(\"\\n!!!Unexpectedly long variable name\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "violent-potter",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# View categories for sanity\n",
    "\n",
    "# Check unique data sources\n",
    "print(\"\\nSources(\" + str(len(vars_df[src_tag].unique())) + \"):\")\n",
    "for name in vars_df[src_tag].unique():\n",
    "    print(name)\n",
    "    \n",
    "# Check unique dataset filenames (ignoring visit suffix)\n",
    "sas_df = set()\n",
    "for df in vars_df[sas_ds_tag].unique():\n",
    "    if df[-2:] in ['00','01','02','03','04','05','06','07','08','09','10','11','99']:\n",
    "        sas_df.add(df[:-2])\n",
    "    else:\n",
    "        sas_df.add(df)\n",
    "sas_df = list(sas_df)\n",
    "sas_df.sort()\n",
    "print(\"\\nDataset files(\" + str(len(sas_df)) + \"):\")\n",
    "for df in sas_df:\n",
    "    print(df)\n",
    "if len(sas_df) > 22:\n",
    "    print(\"\\n!!!Unexpected number of dataset file names\")\n",
    "    \n",
    "# Check all categories \n",
    "cats = list(vars_cat_df[cat_tag].unique())\n",
    "cats.sort()\n",
    "print(\"\\nCategories(\" + str(len(cats)) + \"):\")\n",
    "for c in cats:\n",
    "    print(c)\n",
    "if len(cats) > 32:\n",
    "    print(\"\\n!!!Unexpected number of category types\")\n",
    "    \n",
    "# Check unique categories in release comments\n",
    "rcs = list(vars_df[rc_tag].unique())\n",
    "rcs.remove(np.nan)\n",
    "rcs.sort()\n",
    "print(\"\\n\" + rc_tag + \"(\" + str(len(rcs)) + \"):\")\n",
    "for rc in rcs:\n",
    "    print(rc)\n",
    "if len(rcs) > 4:\n",
    "    print(\"\\n!!!Unexpected number of release comment types\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intellectual-story",
   "metadata": {},
   "source": [
    "## Grouping variables by category/subcategory and visit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perceived-aberdeen",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the visit prefix from the variable names to group data by cat/subcat/visit\n",
    "# This is just to get a sense of things\n",
    "tmp_df = vars_cat_df.copy()\n",
    "tmp_df[\"Visit\"] = tmp_df[var_tag].str[:3]\n",
    "for visit in visits.keys():\n",
    "    tmp_df[visit] = np.where(tmp_df['Visit'] == visit, True, False)\n",
    "tmp_df = tmp_df.drop(columns=[var_tag, 'Visit'])\n",
    "tmp_df = tmp_df.groupby([cat_tag, sub_tag], observed=True).sum()\n",
    "\n",
    "pandas.set_option('display.max_rows', None)\n",
    "display(tmp_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emotional-helena",
   "metadata": {},
   "source": [
    "### Dump variable count per cat/subcat and visit into a wiki table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amateur-tiger",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump categories and subcategories into a wiki table\n",
    "# Table lists categories and subcategories and which visits collected any data related to that subcategory\n",
    "\n",
    "table_str = '{| class=\"wikitable\"\\n ! Category !! Subcategory !! ' + ' !! '.join(visits.values())\n",
    "last_cat = \"\"\n",
    "for group, sub in tmp_df.index:\n",
    "    table_str += \"\\n|-\\n\"\n",
    "    if group != last_cat:\n",
    "         table_str += \"|rowspan=\" + str(len(tmp_df.loc[group].index)) + \" | \" + group + '\\n'\n",
    "    table_str += \"| \" + sub\n",
    "    for col in visits:\n",
    "        table_str += \" || \" + str(tmp_df.loc[group].loc[sub][col])\n",
    "    last_cat = group\n",
    "table_str += \"\\n|}\"\n",
    "\n",
    "print(table_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medieval-truth",
   "metadata": {},
   "source": [
    "## Grouping variables by variable source and visit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dramatic-colony",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tmp_df = vars_df.copy()\n",
    "tmp_df[\"Visit\"] = tmp_df[var_tag].str[:3]\n",
    "for visit in visits.keys():\n",
    "    tmp_df[visit] = np.where(tmp_df['Visit'] == visit, True, False)\n",
    "tmp_df = tmp_df.drop(columns=[var_tag, 'Visit'])\n",
    "tmp_df = tmp_df.groupby([src_tag], observed=True).sum()\n",
    "\n",
    "pandas.set_option('display.max_rows', None)\n",
    "display(tmp_df[tmp_df.columns.difference(['Page'])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complete-montreal",
   "metadata": {},
   "source": [
    "### Dump variable counts per source and visit into a wiki table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporated-dayton",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table lists sources and which visits collected any data related to that source\n",
    "\n",
    "table_str = '{| class=\"wikitable\"\\n! Source !! ' + ' !! '.join(visits.values())\n",
    "last_src = \"\"\n",
    "for src in tmp_df.index:\n",
    "    table_str += \"\\n|-\\n\"\n",
    "    if src != last_src:\n",
    "        table_str += \"| \" + src\n",
    "    for col in visits:\n",
    "        table_str += \" || \" + str(tmp_df.loc[src].loc[col])\n",
    "    last_cat = src\n",
    "table_str += \"\\n|}\"\n",
    "\n",
    "print(table_str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
