{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "innocent-content",
   "metadata": {},
   "source": [
    "# Parse the VG_Form.pdf for OAI variable categories and sources\n",
    "\n",
    "OAI has a number of PDFs that list all variables collected, their nature, and where they came from. Most handy is General Information.zip\\/General\\/VG_Form.pdf as we can use this to get the categories and subcategories.\n",
    "\n",
    "With over 9,000 variables collected, getting the categories can be critical just to start to make sense of the variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authentic-replica",
   "metadata": {},
   "source": [
    "## Imports and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dated-credit",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandong.hill/.pyenv/versions/3.9.6/envs/oai-data/lib/python3.9/site-packages/pandas/compat/__init__.py:124: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pickle\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pdfminer.high_level import extract_pages\n",
    "from pdfminer.layout import LTTextContainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "reliable-ordering",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "# Tags to look for when parsing \n",
    "hdr_tag = \"Variable Guide\"\n",
    "vn_tag = \"Variable Name\"\n",
    "var_tag = \"Variable\"\n",
    "src_tag = \"Source\"\n",
    "page_tag = \"Page\"\n",
    "lbl_tag = \"Label\"\n",
    "sas_ds_tag = \"SAS Dataset\"\n",
    "rc_tag = \"Release Comments\"\n",
    "cat_tag = \"Category\"\n",
    "sub_tag = \"Subcategory\"\n",
    "stats1_tag = \"N \"\n",
    "stats2_tag = \"Value N\"\n",
    "end_tag = \"_______________\"\n",
    "ftr_tag = \"Release Version\"\n",
    "\n",
    "# This list was created by first parsing and collecting all listed categories into a set\n",
    "known_categories = [\"Bookkeeping\", \"Demographics\", \"Study eligibility\", \"Knee symptoms\",\n",
    "                    \"Medical history, arthritis\", \"Medications\", \"Physical activity\",\n",
    "                    \"Knee pain/OA status\", \"Other joint symptoms\", \"Back pain\",\n",
    "                    \"Anthropometry\", \"Hand and/or foot exam\", \"Medical history, general\", \n",
    "                    \"Image Assessments: X-ray\", \"Joint imaging\", \"Health care access\",\n",
    "                    \"Global function/disability/QOL\", \"Nutrition\", \"WOMAC/KOOS\", \n",
    "                    \"Knee function/QOL\", \"Blood pressure & pulse\", \"Performance measures\", \n",
    "                    \"Strength measures\", \"Knee exam\", \"Biospecimens collection\",\n",
    "                    \"Image Assessments: MRI\", \"Accelerometry\", \"Hip function/QOL\", \n",
    "                    \"Hip symptoms\", \"Outcomes\"]\n",
    "\n",
    "visits = {'P02':'IEI', 'P01':'SV', 'V00':'EV', 'V01':'12m', 'V02':'18m', 'V03':'24m', 'V04':'30m', 'V05':'36m', 'V06':'48m', 'V07':'60m', 'V08':'72m', 'V09':'84m', 'V10':'96m', 'V11':'108m', 'V99':\"Outcomes\"}\n",
    "\n",
    "col_names = [var_tag, lbl_tag, src_tag, page_tag, sas_ds_tag, rc_tag]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupational-tradition",
   "metadata": {},
   "source": [
    "## Original file ingestion\n",
    "\n",
    "Parse the PDF once, and convert into a smaller serialized format for faster re-processing in the future (approx 10 min to read the whole PDF). Do this once, and afterwards only use the binary reader in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lyric-vegetarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse PDF into a list of pages\n",
    "# Each page is only a list of the PDF textboxes in the page\n",
    "\n",
    "pages = []\n",
    "for page_layout in tqdm(extract_pages(r\"../data/pdfs/General/VG_Form.pdf\")):\n",
    "    text_boxes = []\n",
    "    for element in page_layout:\n",
    "        if isinstance(element, LTTextContainer):\n",
    "            text_boxes.append((element.get_text(), element.x0, element.y0, element.x1, element.y1))\n",
    "    pages.append(text_boxes)\n",
    "\n",
    "pickle.dump(pages, open( \"pkl/vg_form_pdf_elements.pkl\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "timely-parish",
   "metadata": {},
   "source": [
    "# Fast file ingestion\n",
    "Read in the binary serialized data (approx 1 sec to read the binary form) created in prior section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "distinguished-milan",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = pickle.load(open(\"pkl/vg_form_pdf_elements.pkl\", \"rb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streaming-customs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many variables are described in this document? (for a later sanity check)\n",
    "total_var_cnt = 0\n",
    "for page in pages:\n",
    "    for element in page:\n",
    "        if element[0].strip().startswith(end_tag):\n",
    "            total_var_cnt += 1\n",
    "print(total_var_cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conditional-technical",
   "metadata": {},
   "source": [
    "## Ensure the PDF textboxes are in the same order they are rendered on a page "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedicated-botswana",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse PDF elements into a list of text boxes and coordinates that match their rendering order\n",
    "\n",
    "lines = []\n",
    "for page in tqdm(pages):\n",
    "    text_boxes = []\n",
    "    for element in page:\n",
    "        txt = element[0].strip()\n",
    "        \n",
    "        # found text box containing a header or footer, ignore and move on\n",
    "        if txt.startswith(hdr_tag) or txt.startswith(page_tag) or txt.startswith(ftr_tag):\n",
    "            continue\n",
    "        \n",
    "        # Check for multiple lines in a single text_box\n",
    "        txt = txt.split('\\n')\n",
    "        if len(txt) == 1:\n",
    "            text_boxes.append((txt[0].strip(), element[2], element[4], element[1])) # txt, y0, y1, x0\n",
    "        else:\n",
    "            # Delete dups: This PDF has random text repeated in the PDF even though it only renders them once\n",
    "            tmp = []\n",
    "            [tmp.append(x.strip()) for x in txt if x not in tmp]\n",
    "            txt = tmp\n",
    "            \n",
    "            # Break multiline boxes into single line boxes\n",
    "            line_cnt = len(txt)\n",
    "            height = (element[4] - element[2])/line_cnt # y1-y0\n",
    "            for i in range(line_cnt):\n",
    "                text_boxes.append((txt[i], element[2]+(height*(line_cnt-(1+i))), element[4]-(height*i), element[1]))\n",
    "\n",
    "    # Order text_boxes by vertical (top to bottom), then horizontal position (left to right)                \n",
    "    text_boxes = sorted(text_boxes, key=lambda box: box[3])  # secondary sort variable (horiz pos)\n",
    "    text_boxes = sorted(text_boxes, key=lambda box: box[2], reverse=True) # primary sort variable (vert pos)\n",
    "\n",
    "    # Concatenate text_boxes that render at same horizontal position (shows up in the pdf as Label: value)\n",
    "    last_tb = [\"\", sys.maxsize, sys.maxsize, sys.maxsize]\n",
    "    for tb in text_boxes:\n",
    "        if (last_tb[1] - tb[1]) < 1:  # same line of text on a page\n",
    "             lines[-1] = lines[-1] + \" \" + tb[0].strip()\n",
    "        else:  # new line of text\n",
    "            lines.append(tb[0].strip())\n",
    "        last_tb = tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjacent-settle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional memory cleanup\n",
    "text_boxes = None\n",
    "pages = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinate-entrance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kludge: there are a few text boxes that place the text None a line before Release Comments,\n",
    "# yet renders them on the same line as Release Comments: None\n",
    "# This hunts those down and fixes them before parsing\n",
    "l = 0\n",
    "while l < len(lines):\n",
    "    if lines[l] == \"None\" and lines[l+1].startswith(rc_tag):\n",
    "        lines[l] = lines[l+1] + \" \" + lines[l]\n",
    "        lines.pop(l+1)\n",
    "    l += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "refined-oxford",
   "metadata": {},
   "source": [
    "## Parse text of the document into data about OAI variables\n",
    "First parse into lists, then into pandas dataframes.\n",
    "One dataframe includes each variable, label, source, source page, dataet file name, and release comment.\n",
    "Since each variable can have more than one category/subcategory associated with it, a separate dataframe is used to hold these associations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressed-postcard",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the text into data about the OAI variables\n",
    "\n",
    "def get_var(lines, l, label, next_label=None):\n",
    "    # Parse as many lines as needed to get the variable value\n",
    "    # Return value and new parser location\n",
    "    assert(lines[l].startswith(label))\n",
    "    value = lines[l][len(label)+1:].strip()\n",
    "    l += 1\n",
    "    # Some comments run beyond one line\n",
    "    while next_label and not lines[l].startswith(next_label):\n",
    "        value = value + \" \" + lines[l].strip()\n",
    "        l += 1\n",
    "    return value, l\n",
    "    \n",
    "\n",
    "variables = []\n",
    "variable_cats = [] \n",
    "l = 0\n",
    "while l < len(lines):\n",
    "    # The first line describes where the variable came from\n",
    "    src_page = None\n",
    "    src = lines[l].strip()\n",
    "    # If applicable, split out src and src_page\n",
    "    idx = re.search(page_tag + \"|\" + page_tag.lower() + \"| p |, p\", lines[l])\n",
    "    if idx:\n",
    "        src = lines[l][:idx.start()].strip()\n",
    "        src_page = lines[l][idx.end():].strip()\n",
    "    else:\n",
    "        idx = re.search(\"p\\d+\", lines[l]) # e.g. p50Q\n",
    "        if idx:\n",
    "            src = lines[l][:idx.start()].strip()\n",
    "            src_page = lines[l][idx.start()+1:].strip()\n",
    "\n",
    "    # Clean up source names that are different just because of spelling\n",
    "    src = src.replace(\"Follow-Up\", \"Follow-up\")\n",
    "    src = re.sub(\"Quest$\", \"Questionnaire\", src)\n",
    "    src = src.replace(\"Checklist\", \"\").strip()\n",
    "    \n",
    "    l += 1\n",
    "    \n",
    "    # Next is the variable name, never larger than a single line \n",
    "    var_name, l = get_var(lines, l, vn_tag)\n",
    "    \n",
    "    # Next is the variable label\n",
    "    label, l = get_var(lines, l, lbl_tag, sas_ds_tag)\n",
    "    \n",
    "    # Get the name of the SAS Dataset the variable is stored in\n",
    "    dataset, l = get_var(lines, l, sas_ds_tag)\n",
    "    \n",
    "    # Get the release comments\n",
    "    rel_cmnts, l = get_var(lines, l, rc_tag, cat_tag)\n",
    "    \n",
    "    # Get the categories/subcategories\n",
    "    assert(lines[l].startswith(cat_tag))\n",
    "    l += 1 # desired values are the line below the text \"Category:\"\n",
    "    while not lines[l].startswith(stats1_tag) and not lines[l].startswith(stats2_tag) and not lines[l].startswith(end_tag):\n",
    "        for cat in known_categories:\n",
    "            if lines[l].startswith(cat):\n",
    "                # Store as name, category, subcategory (which is always on the same line as the category)\n",
    "                variable_cats.append((var_name, cat, lines[l][len(cat):].strip()))\n",
    "                break\n",
    "        l += 1\n",
    "    \n",
    "    # All remaining text is the summary stats section (ignored), and the line marks the end of a variable description\n",
    "    while l < len(lines) and not lines[l].startswith(end_tag):\n",
    "        l += 1\n",
    "    l += 1\n",
    "    \n",
    "    variables.append((var_name, label, src, src_page, dataset, rel_cmnts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "external-scenario",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional memory cleanup\n",
    "lines = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confident-peninsula",
   "metadata": {},
   "source": [
    "## Clean and Save Variable Data In Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selective-castle",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Put variable data into Pandas dataframe, and optimize the storage (reduces to 1/3 size)\n",
    "vars_df = pd.DataFrame(variables, columns=col_names)\n",
    "# Setting types, reduces memory size by 50%\n",
    "vars_df[var_tag] = vars_df[var_tag].astype('string')\n",
    "vars_df[lbl_tag] = vars_df[lbl_tag].astype('string')\n",
    "vars_df[src_tag] = vars_df[src_tag].astype('category')\n",
    "vars_df[page_tag] = vars_df[page_tag].astype('category')\n",
    "vars_df[sas_ds_tag] = vars_df[sas_ds_tag].astype('category')\n",
    "vars_df[rc_tag].replace('None', np.nan, inplace=True)\n",
    "vars_df[rc_tag] = vars_df[rc_tag].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conditional-membrane",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity checks\n",
    "print('Variables in dataframe: ' + str(vars_df.shape[0]))\n",
    "assert vars_df.shape[0] == (total_var_cnt)\n",
    "\n",
    "# NA is the expected columns?\n",
    "for cn in col_names:\n",
    "    na_cnt = vars_df[cn].isna().sum()\n",
    "    if na_cnt > 0 and cn not in [page_tag, rc_tag]:\n",
    "        print('\\n!!!Unexpected NA values in column: ' + cn)\n",
    "\n",
    "# Are the source page numbers sane? e.g. 4 (extension)\n",
    "if vars_df.Page.map(len).max() > 13:\n",
    "    print('\\n!!!Unexpectedly large page number: ' + str(vars_df.Page.max()))\n",
    "\n",
    "# Are the variable names sane?\n",
    "if vars_df.Variable.map(len).max() > 11:\n",
    "    print('\\n!!!Unexpectedly long variable name')\n",
    "\n",
    "# View values for sanity\n",
    "# Check unique data sources\n",
    "print('\\nSources(' + str(len(vars_df.Source.unique())) + '):')\n",
    "for name in vars_df.Source.unique():\n",
    "    print(name)\n",
    "    \n",
    "# Check unique dataset filenames (ignoring visit suffix)\n",
    "sas_df = set()\n",
    "for df in vars_df[sas_ds_tag].unique():\n",
    "    if df[-2:] in ['00','01','02','03','04','05','06','07','08','09','10','11','99']:\n",
    "        sas_df.add(df[:-2])\n",
    "    else:\n",
    "        sas_df.add(df)\n",
    "sas_df = list(sas_df)\n",
    "sas_df.sort()\n",
    "print('\\nDataset files(' + str(len(sas_df)) + '):')\n",
    "for df in sas_df:\n",
    "    print(df)\n",
    "if len(sas_df) > 22:\n",
    "    print('\\n!!!Unexpected number of dataset file names')\n",
    "    \n",
    "# Check unique categories in release comments\n",
    "rcs = list(vars_df[rc_tag].unique())\n",
    "rcs.remove(np.nan)\n",
    "rcs.sort()\n",
    "print('\\n' + rc_tag + '(' + str(len(rcs)) + '):')\n",
    "for rc in rcs:\n",
    "    print(rc)\n",
    "if len(rcs) > 4:\n",
    "    print('\\n!!!Unexpected number of release comment types')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visible-invitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional cleanup\n",
    "variables = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "therapeutic-prison",
   "metadata": {},
   "source": [
    "### Review/cleanup results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brutal-stone",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  A quick look at the variable parse results\n",
    "\n",
    "# print(str(vars_df.memory_usage(index=True).sum()) + \" bytes\")\n",
    "vars_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessible-steam",
   "metadata": {},
   "source": [
    "From this we see that 27 variables are listed twice.\n",
    "\n",
    "Labels are repeated because most labels don't encode the visit data, and the same questions get asked at different visits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prospective-excerpt",
   "metadata": {},
   "source": [
    "### Remove repeated variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "useful-being",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at repeated variables\n",
    "vars_df.loc[vars_df.Variable.duplicated(keep=False)].sort_values(by=['Variable'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "light-subscription",
   "metadata": {},
   "source": [
    "There seem to be 27 variables names that are repeated. Each repeated variable has a source listed as \"Follow-up Visit Interview/Workbook\" and a twin with a source \"96-Month Close-Out/108-Mo Invw\" (or something similar).\n",
    "\n",
    "I haven't gone through all the data yet, but I'm not convinced yet that source documents are extremely accurately recorded. I can't see any reason a small handful of questions are marked '96-Month Close-Out/108-Mo Invw' or '96-Month Close-Out Follow-up Intvw'. In fact, only one 96m variable is marked with '96-Month Close-Out/108-Mo Invw', the interview date.  This seems like more human inconsistence in bookkeeping. For now, let's fix this by reducing these sources to the common label \"Follow-up Visit Interview\". Note that the source pages for the duplicates are lost in this.\n",
    "\n",
    "Feel free to fix that if you need it. Ideally, sources would replaced with the names of the PDF files in the CRFs Workbooks.zip archive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "third-worcester",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing dropping repeated values.\n",
    "vars_df = vars_df.drop(vars_df.loc[vars_df.Variable.duplicated(keep=False) & (vars_df.Source == '96-Month Close-Out/108-Mo Invw')].index)\n",
    "vars_df = vars_df.drop(vars_df.loc[vars_df.Variable.duplicated(keep=False) & (vars_df.Source == '96-Month Close-Out Follow-up Intvw')].index)\n",
    "# Drop the category (this does mix up the source pages)\n",
    "vars_df.Source = vars_df.Source.str.replace('96-Month Close-Out/108-Mo Invw', 'Follow-up Visit Interview')\n",
    "vars_df.Source = vars_df.Source.str.replace('96-Month Close-Out Follow-up Intvw', 'Follow-up Visit Interview')\n",
    "vars_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civilian-twins",
   "metadata": {},
   "source": [
    "### Clean labels\n",
    "\n",
    "We already track the visit information in the variable name, and the variable source in its own column, there is no need for either to be appended to the label. Doing so prevents us from knowing how many unique variables there are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atmospheric-correlation",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_prefixes = ['SV', 'SV/EV XR', 'IEI', 'EV', 'EV WBK', 'EV XR', 'EV MRI', 'SAQ',\n",
    "                  'BL kXR reading \\(JD\\)', 'BL/FU kXR reading', 'BL/FU kXR reading \\(JD\\)', 'BL/FU kXR reading \\(BU\\)',\n",
    "                  'BL/FU kMRI reading \\(BI\\)', 'BL/FU kMRIreading \\(BI\\)', 'BL/FU kMRI reading \\(FE\\)', 'BL/FU kMRI reading \\(VS\\)',\n",
    "                  'FU flXR reading \\(DC\\)', 'FU flXR reading \\(JD\\)',\n",
    "                  'FU INT', 'FU WKB', 'FU WBK', 'FU SAQ', 'FU MRI', 'FU XR',\n",
    "                  'Accelerometry', 'MISSED', 'Enr Expn', 'Outcomes']\n",
    "\n",
    "# Look at how often each prefix is used\n",
    "total = 0\n",
    "for pre in label_prefixes:\n",
    "    count = vars_df.Label.str.count(r'^' + pre + ':').sum()\n",
    "    total += count\n",
    "    print(pre + '\\t' + str(count))\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addressed-stupid",
   "metadata": {},
   "source": [
    "Clearly, existing labels have typos and the prefixes aren't consistent.\n",
    "\n",
    "Some simple examples:\n",
    "* SV/EV XR - no data with this label is part of the SV datasets (all variables start with V00)\n",
    "* EV vs EV WBK - 499 variables with the former, 1 with the latter prefix\n",
    "* FU WKB vs FU WBK - 15 with the former label, 1600 with the latter prefix\n",
    "* BL/FU kMRIreading (BI) - clearly  a typo shared by 5 variables\n",
    "* SAQ - mostly items from the V00 visit, but 3 variables with this prefix are from later visits\n",
    "\n",
    "Step one: Remove these prefixes, they don't seem to add any information that isn't stored elsewhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "difficult-gilbert",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pre in label_prefixes:\n",
    "    vars_df.Label = vars_df.Label.str.replace(r'^' + pre + ':','')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "union-compiler",
   "metadata": {},
   "source": [
    "Depending on the visit, the same question may have a different question number. Knowing the question number may be handy, but put it in its own data field. We are trying to track similar questions across visits and these question number prefixes obscure that.\n",
    "\n",
    "Step two: remove the question number prefixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anticipated-dictionary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I haven't found if the asterisk in a label has any meaning, remove for now (2000+ variables)\n",
    "vars_df.Label = vars_df.Label.str.lstrip('*')\n",
    "\n",
    "# Pull question label into its own column (note that 2 variables don't start the question label with Q, grr)\n",
    "vars_df['Question'] = vars_df.Label.str.extract(r'(^[Q]?[D]?\\d+[a-zA-Z\\(\\)0-9]*[\\. ])')[0].str.rstrip('.')\n",
    "vars_df.Question = vars_df.Question.str.strip()\n",
    "vars_df.Label = vars_df.Label.str.replace(r'(^[Q]?[D]?\\d+[a-zA-Z\\(\\)0-9]*[\\. ])','')\n",
    "vars_df.Label = vars_df.Label.str.strip()\n",
    "vars_df.Question = vars_df.Question.astype('category')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "illegal-radical",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are the question numbers sane?\n",
    "if vars_df.Question.map(len).max() > 7:\n",
    "    print('\\n!!!Unexpectedly long question number')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "square-documentary",
   "metadata": {},
   "source": [
    "Step three: correct some obviousl label typos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "associate-puppy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct typos in labels (found while writing other scripts)\n",
    "vars_df.Label = vars_df.Label.str.replace('Isometric Strength', 'Isometric strength')\n",
    "vars_df.Label = vars_df.Label.str.replace('RA Symptoms', 'RA symptoms')\n",
    "vars_df.Label = vars_df.Label.str.replace(r'^$','Left knee difficulty: in car/out of car, last 7 days') # The label was blank on V01-V11DILKN7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressed-berlin",
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_df.Label.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automated-extreme",
   "metadata": {},
   "source": [
    "Only roughly 2,500 variables are unique. That is easier than 9,250."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dependent-tracker",
   "metadata": {},
   "source": [
    "### Save the variable data\n",
    "Saving as a Pandas dataframe for quick loading in other notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "future-monkey",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(vars_df, open('pkl/oai_vars_labels_sources.pkl', 'wb' ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subjective-petersburg",
   "metadata": {},
   "source": [
    "## Clean and Save Category Data In Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gentle-incentive",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put categorical data into Pandas dataframe, and optimize the storage (reduces to 1/3 size)\n",
    "vars_cat_df = pd.DataFrame(variable_cats, columns=[var_tag, cat_tag, sub_tag])\n",
    "vars_cat_df[var_tag] = vars_cat_df[var_tag].astype('string')\n",
    "vars_cat_df[cat_tag] = vars_cat_df[cat_tag].astype('category')\n",
    "vars_cat_df[sub_tag] = vars_cat_df[sub_tag].astype('category')\n",
    "vars_cat_df = vars_cat_df.drop_duplicates() # make sure to drop the duplicates from the parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sapphire-warrior",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  A quick look at the category/subcat parse results\n",
    "# print(str(vars_cat_df.memory_usage(index=True).sum()) + \" bytes\")\n",
    "vars_cat_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharp-engineering",
   "metadata": {},
   "source": [
    "Many variables have more than one category/subcategory assigned to them, so the count being greater than 9250 is expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parliamentary-stand",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View categories for sanity\n",
    "    \n",
    "# Check all categories \n",
    "cats = list(vars_cat_df.Category.unique())\n",
    "cats.sort()\n",
    "print('\\nCategories(' + str(len(cats)) + '):')\n",
    "for c in cats:\n",
    "    print(c)\n",
    "if len(cats) > 32:\n",
    "    print('\\n!!!Unexpected number of category types')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "negative-british",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional cleanup\n",
    "variable_cats = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fewer-bishop",
   "metadata": {},
   "source": [
    "### Save the category data\n",
    "Saving as a Pandas dataframe for quick loading in other notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imported-saint",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(vars_cat_df, open('pkl/oai_vars_categories_subcategories.pkl', 'wb' ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}